# Generator Service

Сервис генерации ответов на основе поисковой выдачи. Принимает поисковый запрос и результаты поиска, генерирует осмысленный ответ с использованием локальной LLM модели.

## Назначение

Generator Service является компонентом RAG (Retrieval-Augmented Generation) системы. Он получает от websearch результаты поиска по пользовательскому запросу и генерирует на их основе осмысленный ответ, опираясь на системный промпт и контекст из поисковой выдачи.

## Функциональность

- Получение поискового запроса и результатов поиска через gRPC
- Использование локальной LLM модели (через llama.cpp) для генерации ответов
- Формирование ответа на основе предоставленного контекста
- Возврат сгенерированного ответа с информацией об источниках
- Поддержка настраиваемых параметров генерации (температура, длина ответа и т.д.)
- Интеграция с системой логирования для отслеживания производительности и ошибок

## Структура файлов

```
generator/
├── generator.py              # Основной файл сервиса генерации
├── generator.proto           # Определение gRPC интерфейса
├── generator_pb2.py          # Сгенерированные protobuf сообщения
├── generator_pb2_grpc.py     # Сгенерированный gRPC код
├── config.json               # Файл конфигурации
├── README.md                 # Документация сервиса
├── generator.log             # Файл логов (генерируется автоматически)
├── models/                   # Директория для хранения моделей
│   └── model.gguf            # GGUF файл модели (пример)
└── requirements.txt          # Зависимости проекта
```

## Архитектура

```
[WebSearch] -> [Generator] -> [LLM (llama.cpp)]
```

## Протоколы коммуникации

- Внутренняя коммуникация: gRPC
- Интерфейс: GeneratorService с методом Generate

## Конфигурация

Конфигурация осуществляется через файл `config.json`:

```json
{
  "server": {
    "host": "[::]",
    "port": 50056,
    "max_workers": 4
  },
  "model": {
    "path": "./models/model.gguf",
    "n_ctx": 2048,
    "n_threads": 4,
    "n_gpu_layers": 0,
    "temperature": 0.7,
    "top_p": 0.9,
    "repeat_penalty": 1.1,
    "max_tokens": 1024
  },
  "system_prompt": "Вы являетесь помощником в предоставлении ответов на вопросы на основе предоставленных документов. Отвечайте точно и кратко, опираясь только на информацию, содержащуюся в документах.",
  "logging": {
    "level": "INFO",
    "file": "generator.log",
    "format": "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
  }
}
```

### Параметры сервера:
- `host`: IP-адрес, на котором будет запущен gRPC сервер
- `port`: порт, на котором будет работать сервис
- `max_workers`: максимальное количество рабочих потоков для обработки запросов

### Параметры модели:
- `path`: путь к GGUF-модели
- `n_ctx`: размер контекста модели
- `n_threads`: количество потоков CPU
- `n_gpu_layers`: количество слоев для GPU (0 для CPU)
- `temperature`: температура генерации
- `top_p`: параметр top-p для генерации
- `repeat_penalty`: штраф за повторения
- `max_tokens`: максимальное количество токенов в ответе

### Параметры логирования:
- `level`: уровень логирования (DEBUG, INFO, WARNING, ERROR)
- `file`: имя файла для сохранения логов
- `format`: формат строки лога

## Зависимости

- `grpcio` и `grpcio-tools` для gRPC
- `protobuf` для сериализации
- `llama-cpp-python` для работы с LLM
- `concurrent.futures` для многопоточности

## Установка и запуск

1. Установите зависимости:
```bash
pip install grpcio grpcio-tools protobuf llama-cpp-python
```

2. Поместите GGUF-модель в папку `models/` и укажите путь в конфигурации

3. Запустите сервис:
```bash
python generator.py
```

Сервис будет доступен на порту 50056 (по умолчанию).

## Протокол gRPC

Сервис предоставляет метод `Generate`, который принимает:
- `query`: поисковый запрос пользователя
- `search_results`: результаты поиска с текстом и метаданными
- `system_prompt`: системный промпт (опционально, используется из конфига по умолчанию)

И возвращает:
- `answer`: сгенерированный ответ
- `sources`: список источников, использованных для генерации
- `generation_time`: время генерации в секундах

## Настройка

Для корректной работы сервиса необходимо:
1. Установить GGUF-модель в директорию `models/`
2. Указать правильный путь к модели в `config.json`
3. Настроить параметры модели в соответствии с вашими требованиями
4. Убедиться, что порт 50056 (или другой, указанный в конфиге) свободен

## Использование

Сервис работает как часть RAG-системы и обычно вызывается из websearch сервиса при включенной генерации ответов. При прямом вызове через gRPC сервис принимает поисковый запрос и результаты поиска, формирует промпт с учетом системного промпта и контекста, после чего генерирует ответ с использованием локальной LLM модели.